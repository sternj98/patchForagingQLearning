A second pass at Q learning in a patchy foraging environment

- Move away from epsilon greedy: (Done)
  - Plug Q values into Boltzman distribution to get softmax probabilistic behavior ie
    P_stay = 1 / (1 + e^(-beta * (Q[stay] - Q[leave] + b + ka(t-1))

    P_stay = 1 / (1 + e^(-beta * (Q[stay] - Q[leave] + b)
    - (Stable representations of dec variables)

- Use function-based reward integrators to match fitting side of project

env.env_state = {"rewsize" : r ; "N0" : n ; "rews" : []}

DONT NEED THE SOFTMAX !!!!
Model1 : rewsize0 , time
  integratedrew = (env_state["rewsize"] , len(env_state["trial_rews"]))
Model2 : rewsize0 , time_since_lastrew
  time_since = list(reversed(env_state["rews"])).index(env_state["rewsize"])
  integratedrew = (env_state["rewsize"] , time_since)
Model3 : rewsize0 , a(totalrew) - b(time)
  intrew = a * sum(env_state["rews"]) - b * len(env_state["rews"])
  integratedrew = (env_state["rewsize"] , intrew)

- Simplify ITI state to a single state with some penalty... ?


Next steps:
1. Get real trials into the training to prepare for fitting to behavioral data
2. Get modular temperature parameter to see if this improves behavior in Model3
  - the reason we're getting this bias effect is because we're falling into the same integrator slots as in
    beginning of trial
